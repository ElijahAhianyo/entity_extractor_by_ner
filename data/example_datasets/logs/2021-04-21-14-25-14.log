2021-04-21 14:25:14
++++++++++++++++++++++++++++++++++++++++CONFIGURATION SUMMARY++++++++++++++++++++++++++++++++++++++++
 Status:
     mode                 : train
 ++++++++++++++++++++++++++++++++++++++++
 Datasets:
     datasets         fold: data/example_datasets
     train            file: train.csv
     validation       file: dev.csv
     vocab             dir: data/example_datasets/vocabs
     delimiter            : b
     use              bert: True
     checkpoints       dir: checkpoints/datasets_bert-bilsm-crf
     log               dir: data/example_datasets/logs
 ++++++++++++++++++++++++++++++++++++++++
Labeling Scheme:
     label          scheme: BIO
     label           level: 2
     suffixes             : ['ORG', 'PER', 'LOC']
     measuring     metrics: ['precision', 'recall', 'f1', 'accuracy']
 ++++++++++++++++++++++++++++++++++++++++
Model Configuration:
     embedding         dim: 768
     max  sequence  length: 300
     hidden            dim: 200
     CUDA  VISIBLE  DEVICE: -1
     seed                 : 42
 ++++++++++++++++++++++++++++++++++++++++
 Training Settings:
     epoch                : 300
     batch            size: 32
     dropout              : 0.5
     learning         rate: 0.001
     optimizer            : Adam
     checkpoint       name: model
     max       checkpoints: 3
     print       per_batch: 20
     is     early     stop: True
     patient              : 5
++++++++++++++++++++++++++++++++++++++++CONFIGURATION SUMMARY END++++++++++++++++++++++++++++++++++++++++
loading label vocab...
mode: train
loading data...
loading data...
training set size: 23181, validating set size: 4636
++++++++++++++++++++training starting++++++++++++++++++++
epoch:1/300
training batch:    20, loss: 0.14956, precision: 0.965 recall: 0.982 f1: 0.973 accuracy: 0.999 
training batch:    40, loss: 0.15750, precision: 1.000 recall: 0.984 f1: 0.992 accuracy: 0.999 
training batch:    60, loss: 0.19098, precision: 0.974 recall: 0.974 f1: 0.974 accuracy: 0.999 
training batch:    80, loss: 0.21891, precision: 0.962 recall: 0.926 f1: 0.943 accuracy: 0.996 
training batch:   100, loss: 0.30501, precision: 0.983 recall: 0.950 f1: 0.966 accuracy: 0.997 
training batch:   120, loss: 0.47790, precision: 0.940 recall: 0.940 f1: 0.940 accuracy: 0.994 
training batch:   140, loss: 0.01609, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 1.000 
training batch:   160, loss: 0.08279, precision: 0.970 recall: 0.985 f1: 0.977 accuracy: 0.998 
training batch:   180, loss: 0.31586, precision: 0.938 recall: 0.938 f1: 0.938 accuracy: 0.995 
training batch:   200, loss: 0.17740, precision: 0.965 recall: 0.948 f1: 0.957 accuracy: 0.997 
training batch:   220, loss: 0.06315, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 1.000 
training batch:   240, loss: 0.06811, precision: 1.000 recall: 0.981 f1: 0.991 accuracy: 0.999 
training batch:   260, loss: 0.35457, precision: 0.952 recall: 0.952 f1: 0.952 accuracy: 0.997 
training batch:   280, loss: 0.77963, precision: 0.978 recall: 0.865 f1: 0.918 accuracy: 0.984 
training batch:   300, loss: 0.08719, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 1.000 
training batch:   320, loss: 0.08605, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 1.000 
training batch:   340, loss: 0.05569, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 1.000 
training batch:   360, loss: 0.06584, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 1.000 
training batch:   380, loss: 0.26340, precision: 0.985 recall: 0.970 f1: 0.977 accuracy: 0.998 
training batch:   400, loss: 0.30678, precision: 0.984 recall: 0.952 f1: 0.968 accuracy: 0.997 
training batch:   420, loss: 0.29291, precision: 0.983 recall: 0.967 f1: 0.975 accuracy: 0.996 
training batch:   440, loss: 0.25298, precision: 0.966 recall: 0.949 f1: 0.957 accuracy: 0.997 
training batch:   460, loss: 0.16523, precision: 0.968 recall: 0.968 f1: 0.968 accuracy: 0.996 
training batch:   480, loss: 0.34804, precision: 0.967 recall: 0.983 f1: 0.975 accuracy: 0.993 
training batch:   500, loss: 0.37247, precision: 0.960 recall: 0.973 f1: 0.966 accuracy: 0.992 
training batch:   520, loss: 0.50866, precision: 0.940 recall: 0.940 f1: 0.940 accuracy: 0.995 
training batch:   540, loss: 0.26997, precision: 0.965 recall: 1.000 f1: 0.982 accuracy: 0.997 
training batch:   560, loss: 0.13397, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 0.997 
training batch:   580, loss: 0.49367, precision: 0.942 recall: 0.953 f1: 0.947 accuracy: 0.993 
training batch:   600, loss: 0.12316, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 1.000 
training batch:   620, loss: 0.29959, precision: 0.918 recall: 0.957 f1: 0.938 accuracy: 0.995 
training batch:   640, loss: 0.21809, precision: 1.000 recall: 0.982 f1: 0.991 accuracy: 0.997 
training batch:   660, loss: 0.10755, precision: 1.000 recall: 0.974 f1: 0.987 accuracy: 0.999 
training batch:   680, loss: 0.06985, precision: 1.000 recall: 1.000 f1: 1.000 accuracy: 1.000 
training batch:   700, loss: 0.45873, precision: 0.981 recall: 0.981 f1: 0.981 accuracy: 0.990 
training batch:   720, loss: 0.08958, precision: 0.976 recall: 0.976 f1: 0.976 accuracy: 0.998 
start evaluate engines...
label: ORG, precision: 0.886 recall: 0.891 f1: 0.883 accuracy: 0.000 
label: PER, precision: 0.964 recall: 0.973 f1: 0.966 accuracy: 0.000 
label: LOC, precision: 0.921 recall: 0.937 f1: 0.926 accuracy: 0.000 
time consumption:295.85(min), precision: 0.937 recall: 0.947 f1: 0.941 accuracy: 0.992 
saved the new best model with f1: 0.941
epoch:2/300
